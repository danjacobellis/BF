{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3a95f0a-c11e-43a4-8ac7-a50518df3d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from fastprogress.fastprogress import master_bar, progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef45ffef-d3a0-48c5-bd8f-07916437d906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SequenceModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SequenceModel, self).__init__()\n",
    "        self.num_positions = 9\n",
    "        self.num_embeddings = 1024\n",
    "        self.embedding_dim = 6\n",
    "        \n",
    "        # Embedding layers for each position\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(num_embeddings=self.num_embeddings, embedding_dim=self.embedding_dim) \n",
    "            for _ in range(self.num_positions)\n",
    "        ])\n",
    "        \n",
    "        # 1D Convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=54, out_channels=54, kernel_size=5, stride=4)\n",
    "        self.conv2 = nn.Conv1d(in_channels=54, out_channels=54, kernel_size=5, stride=4)\n",
    "        self.conv3 = nn.Conv1d(in_channels=54, out_channels=54, kernel_size=5, stride=4)\n",
    "        self.conv4 = nn.Conv1d(in_channels=54, out_channels=54, kernel_size=5, stride=4)\n",
    "        self.conv5 = nn.Conv1d(in_channels=54, out_channels=27, kernel_size=3, stride=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(243, 81)\n",
    "        self.fc2 = nn.Linear(81, 24)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Apply embeddings in parallel\n",
    "        embeddings = [self.embeddings[i](x[:, :, i]) for i in range(self.num_positions)]\n",
    "        embeddings = torch.stack(embeddings, dim=3)\n",
    "        \n",
    "        # Reshape: combining the last two dimensions\n",
    "        reshaped = embeddings.view(batch_size, 54, -1)\n",
    "        \n",
    "        # Apply convolutional layers\n",
    "        x = F.relu(self.conv1(reshaped))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        \n",
    "        # Reshape to a flat vector for the fully connected layers\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create the model instance\n",
    "model = SequenceModel()\n",
    "\n",
    "# Example input: a batch of 2 sequences, each of length 1152, with 9-dimensional, 10-bit codes\n",
    "example_input = torch.randint(0, 1024, (2, 1152, 9), dtype=torch.long)\n",
    "\n",
    "# Forward pass\n",
    "output = model(example_input)\n",
    "\n",
    "print(output.shape)  # Expected shape: [2, 24] for batch size of 2 and 24 output classes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
